\section{BitPacker Makes FHE Accelerators Efficient}
\label{sec:implementation}

\figCraterLake

So far we have presented \name's algorithms, with only a high-level description
of why it helps performance.
We now describe these benefits concretely.
\autoref{sec:craterlake} presents our accelerator baseline;
\autoref{sec:benefits} explains how \name benefits performance, energy, and
area; and \autoref{sec:mapping} explains how \name's level management
algorithms map to accelerators.

\subsection{Overview of CKKS accelerators}
\label{sec:craterlake}
\label{sec:accelerators}

CraterLake~\cite{samardzic:isca22:craterlake} and ARK~\cite{kim2022ark} are
the current state-of-the-art CKKS accelerators.
Both systems have many similarities: they feature wide-vector functional units
tailored to the primitive operations of FHE, include large and explicitly
orchestrated on-chip memories (256\,MB in CraterLake, 512\,MB in ARK) to hold
CKKS's enormous ciphertexts, and are carefully optimized to maximize data reuse
and achieve high throughput.
Earlier accelerators exist, but are not as efficient:
BTS~\cite{kim:isca22:bts}, which ARK improves upon, has a similar structure to
ARK but suffers from excessive memory traffic that bottlenecks performance; and
F1~\cite{feldmann:micro21:f1}, which CraterLake improves upon, is a smaller
accelerator that cannot execute programs with bootstrapping efficiently.

% nikola: NTT is a performance optimization
\autoref{fig:craterlake} shows an overview of CraterLake, the accelerator we
use as a baseline.
CraterLake is a 2048-lane vector processor.
Its primitive datatype are residue polynomials, i.e., vectors of narrow,
word-sized coefficients.
CraterLake has six types of vector functional units (FUs).
Four of these are needed for functional completeness:
modular adders and multiplier FUs perform element-wise vector operations;
NTT FUs perform forward and inverse number-theoretic transforms,
needed to multiply polynomials;
and automorphism FUs implement structured permutations needed for homomorphic
rotates.
The other two units are performance optimizations:
the CRB FU performs change-of-RNS-base operations, a common and expensive
operation that encapsulates many structured multiply-accumulates (ARK has a
similar base-conversion or \emph{bConv} FU);
and the KSHGen FU generates some auxiliary data (keyswitch hints) on-chip to
reduce memory traffic.

These accelerators spend most of their area and energy on computation.
For example, in CraterLake, functional units take 50\% of area and 60-80\% of
energy~\cite{samardzic:isca22:craterlake}.
Moreover, the dominant component of functional units is multipliers, which take
70\% of FU area in CraterLake.
Thus, RNS-CKKS's inefficiency has a significant impacts, as we will see below.

These accelerators take different approaches to hardware word width:
CraterLake uses narrow 28-bit words, which requires RNS-CKKS to use multiple residues
(typically two) per CKKS level, whereas ARK uses wider 64-bit words,
so RNS-CKKS uses a single residue per CKKS level.
Both systems suffer from low datapath utilization with
RNS-CKKS, as explained in \autoref{sec:introduction}.
For instance, with a 40-bit scale, \name reduces the number of residues per
ciphertext by 1.4$\times$ and 1.6$\times$ with 28-bit and 64-bit datapaths,
respectively.

%As an example, assume that we use a 40-bit scale.
%In ARK, this leaves 24 out of 64 bits unused, making ciphertexts 1.6$\times$
%larger than needed. This grows on-chip footprint and memory traffic by
%1.6$\times$, and multiplier energy is $1.6^2=2.56$\x.
%CraterLake would require using 40 bits across two 28-bit residues, leaving 16
%out of 56 bits unused, and incurring overheads of 1.4\x and 1.96\x,
%respectively. \todo{This is the nth time we say this. Omit?}

In \autoref{sec:evaluation}, we will see that neither approach is efficient,
and moreover, that \emph{tuning hardware bitwidth within these extremes yields
little savings} across benchmarks that use diverse scales, as every bitwidth is
inefficient for some scales.
\name avoids this problem by packing ciphertexts to fully use the native
hardware words.

%\todo{DANIEL, PLEASE WRITE THIS. Need for fixed-width datapaths: FU performance
%is important, multipliers are hard to fold, and multi-cycle multipliers would
%be dominated by overheads.}

% dsm: I'm not sure this should be in.
\begin{comment}
\paragraph{Why not handle variable word sizes in hardware?}
All prior FHE accelerators use a fixed-width word size, but since we can change the hardware as well,
it's worth asking whether we could change hardware to support multiple word sizes to adapt to RNS-CKKS representation,
instead of using \name.
However, the different approaches to do this would introduce major problems.
A common approach (e.g., in CPUs and GPUs) is to subdivide a wide datapath (e.g., supporting a single 64-bit operation or two 32-bit operations),
but this is still too coarse for our purposes, and makes poor utilization of multipliers (e.g., a single 64-bit multiplier has the area of roughly four 32-bit multipliers).
A more aggressive option would be to use multi-cycle functional units with a very narrow hardware datapath (e.g., 8 bits),
and perform each wide operation over multiple cycle (e.g., taking tens of cycles per multiply).
While this approach is finer-grained, it would suffer from high register overheads,
because modular multiplications use wide intermediate values.
For example, CraterLake's pipelined multipliers already spend 30\% of area on registers
and 70\% on combinational logic. 
Multi-cycle multipliers would be dominated by registers, and since each multiplier would be slower,
significantly more area would be needed for the same throughput.
\end{comment}

\subsection{\name improves performance, energy efficiency, and area}
\label{sec:benefits}

To understand \name's benefits, we discuss how performance and energy change
with the number of residues $R$.
We focus on the cost of a homomorphic multiplication with $N=64K$; homomorphic
rotations have nearly identical costs, and homomorphic additions have
negligible costs.

\paragraph{Performance analysis:}
A single homomorphic multiplication requires $O(R^2)$ multiplies, $O(R^2)$
adds, and $O(R)$ NTTs of residue polynomials (each of which has $N$ elements).
Multiplies and adds are simple, but NTTs are complex (requiring about
16$\times$ more energy than an element-wise multiply).
To achieve high NTT utilization across different values of $R$, the CRB unit
encapsulates most multiplies and adds.
Each use of the CRB unit performs $O(R)$ polynomial multiply-accumulates.
Thus, each FU is used $O(R)$ times per operation, and compute time grows
\emph{linearly} with $R$.

%If the system was completely compute-bound, by reducing $R$, \name would yield 
%a linear improvement in performance for CraterLake.
%However,
Besides compute, memory also adds performance overheads:
ciphertext size is linear with $R$,
so by using smaller ciphertexts,
\name incurs linearly fewer memory stalls than RNS-CKKS.

By combining compute and memory savings, \name improves performance superlinearly.
Since these accelerators seek to balance compute and memory utilization, 
these effects are both important (if the system was either memory- or compute-bound, only one factor would matter).
In practice, we observe that performance is proportional to roughly $R^{1.5}$
on the balanced systems we evaluate (\autoref{sec:evaluation}).

\figHomMult

\paragraph{Energy analysis:}
\autoref{fig:homMult} shows a breakdown of energy per component, including
different functional units and the register file (we assume all operands are
on-chip; memory, which we include later, typically adds minor energy costs).
The CRB and NTT FUs dominate energy, as they perform many operations per
input: $R$ multiply-accumulates per element for the CRB, and $O(\log_2 N)$ multiplies and
adds per element for the NTT.
Overall, \autoref{fig:homMult} shows that energy grows superlinearly with $R$, by about $O(R^{1.6})$, i.e.,
3$\times$ per doubling of $R$.
Growth is sub-quadratic because only the CRB grows quadratically; NTT and
RegFile energy grow linearly.
By reducing $R$, \name yields a superlinear reduction in energy.

\paragraph{Area analysis:}
For a given hardware configuration, two components can be reduced linearly
without loss of performance when moving from RNS-CKKS to \name:
the CRB and the register file.
The CRB is sized to perform $R_{max}$ multiply-adds per element, so by reducing $R_{max}$,
we can reduce the number of multiply-adds per lane on the CRB without loss of performance.
And since ciphertext size is proportional to $R$, by reducing $R$,
the register file needs less space to hold the same number of ciphertexts.
%and main memory needs less bandwidth to transmit them when they miss.
This yields significant savings, since FHE accelerators have large register files,
e.g., taking 40\% of die area in CraterLake~\cite[Table 2]{samardzic:isca22:craterlake}.

%There's a complex relationship between area and performance; for memory-bound applications,
%it may be preferable to use larger on-chip storage and bandwdith, achieving higher performance.
%We explore this relationship in \autoref{sec:evaluation}.

% dsm: Maybe skip this till eval??
%\paragraph{Narrow words are more efficient:} 
%mults and adds don't matter for bit-complexity, but NTTs are linear on $R$, so narrower wins.
%Explain with 28-bit energy breakdown, or if 64-bit is avaiable, show side by side and contrast.

%\name makes using narrow words robust and more precise.

%Overall effect of \name: by reducing $R$, linear perf improvement (due to the CRB), but energy savings on top of this: linear for NTT, quadratic for CRB.
%Area savings could be had by reducing CRB and memory.

\subsection{Mapping \name level management to accelerators}
\label{sec:mapping}

\name's rescale and mod-down procedures, described in \autoref{sec:levelMgmt},
take somewhat more computation than RNS-CKKS's.
However, their overheads are minor, and its main component can leverage the CRB
unit in CraterLake (or the \textit{bConv} unit in ARK) to avoid a performance
penalty.

Specifically, the main kernels are \verb!scaleUp! and \verb!scaleDown!.
\verb!scaleUp! is cheap: it multiplies all residues by a single, precomputed
value.
\verb!scaleDown! has $2k(R-k)$ multiplies of residue polynomials.
where $R$ is the number of residues in the input and $k$ is the number of shed
moduli.
Thus,  \verb!scaleDown! can be 2-3$\times$ costlier than \verb!scaleUp!.

Note that this overhead is minor, compared to the $O(R^2)$ multiplies and adds
needed per homomorphic multiplication, as discussed above.
Furthermore, \verb!scaleDown!'s compute (lines \OK{10--13}) can also be handled
by the CRB, so in CKKS accelerators, scaling down by $k$ residues at a time is
almost as fast as scaling down by a single one.
%
As we will see, \name's level management operations cause overheads comparable
to those of RNS-CKKS.

