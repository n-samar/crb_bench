\section{Evaluation}\label{sec:evaluation}
\vspace{-0.05in}
\subsection{\name with 28-bit hardware words}

\figBitwidthSweep

\autoref{fig:main} compares the execution time of \name and RNS-CKKS across
workloads, when using the default CraterLake configuration with 28-bit words
(lower is better).
\name achieves a gmean \OK{53\%} speedup over RNS-CKKS.

All applications see significant improvements from \name, but speedups are more
pronounced on applications that include smaller scales, which are less
efficient in RNS-CKKS:
SqueezeNet and LogReg, which uses 35-bit scales for application work,
see higher speedups than ResNet-20 and RNN, which use 45-bit scales;
and BS19 variants see higher speedups than BS26, since in RNS-CKKS, BS19's
scales pack worse to 28-bit words.

\autoref{fig:breakdown} compares the energy consumed by 28-bit CraterLake when
running \name and RNS-CKKS, normalized to \name (lower is better).
Trends are similar to performance: \name achieves gmean \OK{66\%} lower energy
than RNS-CKKS.
By improving performance \emph{and} energy efficiency at the same time, \name
reduces energy-delay product (EDP) by \OK{2.54$\times$} over RNS-CKKS
(equivalently, improves performance per Joule by this amount).

\autoref{fig:breakdown} also breaks down energy consumption of each approach by
level management operations (rescale and mod-down), shown in \red{red}, vs.
other operations, shown in \blue{blue}.
Level management costs are small for both \name and RNS-CKKS (5\% and 7\%
gmean, respectively).
Interestingly, in absolute terms, \name level management costs are lower than
RNS-CKKS.
This is because, while \name switches multiple residues per level, RNS-CKKS
uses two residues per level, and suffers from a larger number of residues.
% nikola: what does this mean?
Thus, with 28-bit words, \name's level management is not even adding overheads
to save energy.


\subsection{Comparison across word sizes}
\label{sec:sweep}

To characterize \name's benefits more broadly, we evaluate accelerator variants with
different hardware word sizes.
We evaluate designs with 28- to 64-bit words.
Since changing word size has several complex effects, we perform
\emph{iso-throughput} scaling: as we increase word size, we proportionately \emph{reduce the
number of vector lanes}, so that the total number of bits consumed per cycle
stays roughly constant.
For example, a 30-bit accelerator has twice the vector lanes of the 60-bit
accelerator (we do this scaling by changing CraterLake's lane
groups~\cite{samardzic:isca22:craterlake}).
All designs use the same register file size and memory bandwidth.
We retain the same balance of functional units, except that wider words reduce
$R_{max}$, so we reduce the number of multiply-adds per lane of the CRB unit
linearly (otherwise, the CRB would be overdesigned).
For example, the 30-bit design has a CRB with 56 MAC units per lane, whereas
the 60-bit design uses 28 per lane.

Overall, this scaling strategy maintains roughly the same raw computational
throughput across word sizes, but designs with wider words have more area, because
multipliers scale quadratically.
Specifically, whereas the 28-bit design takes 472mm$^2$ in a commercial 14/12nm
process~\cite{samardzic:isca22:craterlake}, the 64-bit design consumes
557mm$^2$, chiefly due to a larger NTT unit.

\autoref{fig:bitwidthsweep} shows how the execution time ($y$-axis) of \name and RNS-CKKS varies as
we scale word size from 28 to 64 bits ($x$-axis). Each plot shows results for a different application.
\autoref{fig:bitwidthsweep} shows a stark difference in behaviors.
\name maintains \emph{constant performance across word sizes},
because it fully leverages the accelerator's raw throughput, which is constant
given our scaling methodology.

\autoref{fig:bitwidthsweep} also shows that RNS-CKKS always performs
worse than \name.
More importantly, RNS-CKKS has \emph{extremely uneven} performance across word
sizes, with peaks and valleys that are about 2$\times$ apart.
The valleys in RNS-CKKS correspond to points where the word size matches one of
the scales in the program.
For example, ResNet-20 with BS19 bootstrapping uses scales of 45 bits (outside
of bootstrapping) and 30, 52, and 55 bits (during bootstrapping).
The valleys occur at these points because RNS-CKKS matches residue sizes to
scales, and some fraction of the residues (but never all) achieve good datapath
utilization.
Conversely, peaks happen at word sizes that cause poor utilization across the
board, like 40 bits in ResNet-20.

Since different applications use different scales, \autoref{fig:bitwidthsweep}
shows that each benchmark shows different peaks and valleys, so the optimal
word size for RNS-CKKS varies significantly across applications.
\name erases this problem, providing uniformly high utilization for any scale.

\figGmeanSlowdown

To better capture general performance trends, \autoref{fig:gmeanSlowdown} shows
the gmean, maximum, and minimum slowdowns of RNS-CKKS compared to \name across
all benchmarks.
This again highlights that RNS-CKKS is inefficient across word sizes, and
shows that large word sizes are somewhat more affected by poor utilization.
For example, at 64 bits, RNS-CKKS's gmean slowdown is
\OK{1.89$\times$},
vs. \OK{53\%}
at 28 bits.

\figGmeanPerformancePerArea

Finally, \autoref{fig:gmeanPerformancePerArea} shows \name and RNS-CKKS
execution times by unit area, relative to the execution time $\times$ area ratio
of \name with 28-bit words (this metric is the inverse of performance per area).
Recall that, to provide the same raw throughput, designs with wider words use somewhat more area:
the 64-bit accelerator is \OK{18\%} larger than the 28-bit one.
This is why the \name line now trends upwards, and the RNS-CKKS line grows more quickly than in \autoref{fig:gmeanSlowdown}.
Overall, RNS-CKKS at 64 bits has \OK{2.3$\times$} worse performance per area than \name at 28 bits.
This shows that narrower word sizes are more efficient, and that
\name with 28-bit words is the most efficient choice.

\subsection{\name reduces accelerator area}

So far, we have evaluated accelerator configurations that were tuned for RNS-CKKS.
But as \autoref{sec:benefits} explained, since \name uses fewer residues,
we can reduce accelerator area without degrading performance.

First, \autoref{fig:rfSweep} shows gmean execution time as the
accelerator's register file size ($x$-axis) changes.
Results are normalized to \name at 256\,MB.
RNS-CKKS plateaus at 256\,MB but steadily loses performance at lower sizes.
By contrast, \name sees no loss in performance when scaling scratchpad size down
from 256\,MB to 200\,MB.
Even at 150\;MB, \name sees only a 50\% slowdown, while RNS-CKKS slows down
by over 2\x.

Second, as \autoref{sec:benefits} discusses, we find that we can make CraterLake's
CRB \OK{28\%} smaller with no performance regression for \name.

Thus, \name enables using a CraterLake configuration that has 395.5\;mm$^2$ instead of the original 472.3\;mm$^2$,
with no loss in performance, a \OK{19\%} area reduction.

Combining \name's performance, energy, and area improvements, we see that \name
with this configuration improves energy-delay-area product by \OK{3.0$\times$}
over RNS-CKKS on the original CraterLake configuration.

\subsection{\name preserves accuracy}

\name improves performance without compromising precision.
\autoref{tbl:main} shows that \name matches the precision of RNS-CKKS across
workloads.
We run benchmarks on many samples to ensure precision is stable
(all results have 95-th percentile confidence intervals below 1\%).
RNS-CKKS precision results use 64-bit words to sidestep issues regarding
using RNS-CKKS with small residue moduli~\cite{kim2022approximate};
RNS-CKKS has better precision with 64-bit words than with smaller word sizes.
\name precision results use 28-bit words, the choice that limits residue moduli
the most and thus has the highest risk to impact precision;
we observe no accuracy changes with larger word sizes.

\figRfSweep
\input{tables/ag_tbl_main.tex}


\autoref{tbl:main} reports mean and absolute error for each benchmark by
comparing the outputs of each sample with those of unencrypted computation
using 64-bit double-precision floating-point data.
\autoref{tbl:main} reports the average number of error-free mantissa bits, and
the worst case across all outputs.
Average errors are well within the 0.5-bit margin that we set for choosing
moduli, and worst-case errors all match.

